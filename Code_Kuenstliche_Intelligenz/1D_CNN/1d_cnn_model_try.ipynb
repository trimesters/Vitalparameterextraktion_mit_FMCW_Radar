{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08c2fcff-1d84-46b1-abba-387216d87457",
   "metadata": {},
   "source": [
    "# Algorithmus für künstliche Intelligenz mit CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60dc300-a77c-4fbc-9740-a019f619e386",
   "metadata": {},
   "source": [
    "## Libaray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45d3a56a-304d-4e05-a93e-862cd647b9c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import optuna\n",
    "\n",
    "import plotly\n",
    "import plotly.io as pio\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a91d2136-47c8-4570-82ae-6281da1afbae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a6e9b2-9d7d-4bdc-a7fb-17f7e73ba928",
   "metadata": {},
   "source": [
    "## Modell auf der GPU ausführen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ad082b2-e2d6-4794-99ca-754588e54b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4a867a-9cb3-468e-8368-82281bcb3b64",
   "metadata": {},
   "source": [
    "## Modell Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0cd0c4b-47a0-4b99-8b5f-e423919bdcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"1d_cnn_model_try\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae871c7-68b7-43ba-aa9a-44dc5c92d040",
   "metadata": {},
   "source": [
    "## Einlesen der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c974fe44-ee94-49ec-a91b-56e57ca1a7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Load_Data:\n",
    "    \n",
    "    def __init__(self, root_dir = \"\"):\n",
    "        \n",
    "        self.root_dir = root_dir\n",
    "        \n",
    "        self.signal_list_all = []\n",
    "        self.lable_list_all = []\n",
    "        \n",
    "        self.signal_list_80 = []\n",
    "        self.signal_list_20 = []\n",
    "        self.lable_list_80 = []\n",
    "        self.lable_list_20 = []\n",
    "        \n",
    "        self.load_data()\n",
    "        self.sort_data()\n",
    "        \n",
    "    def load_data(self):\n",
    "        \n",
    "        for dirpath, dirnames, filenames in os.walk(self.root_dir):\n",
    "            for filename in filenames:\n",
    "                if filename.endswith(\".npz\"): \n",
    "                    \n",
    "                    file_path = os.path.join(dirpath, filename)\n",
    "                    data = np.load(file_path)\n",
    "\n",
    "                    signal_ = data[\"signal\"]\n",
    "                    signal_full = signal_[np.newaxis] # An additional dimension is needed for later processing\n",
    "                    signal = signal_full[:,::]  # Halving of the input data and thus of the features\n",
    "                    \n",
    "                    heart_rate = data[\"heart_uart\"]\n",
    "                    respiration_rate = data[\"respiration_radar\"]\n",
    "                    \n",
    "                    heart_rate_t = heart_rate\n",
    "                    respiration_rate_t = respiration_rate\n",
    "                    signal_t = signal\n",
    "   \n",
    "                    self.lable_list_all.append([heart_rate_t, respiration_rate_t])\n",
    "                    self.signal_list_all.append(signal_t)\n",
    "    \n",
    "    def sort_data(self):   \n",
    "        \n",
    "        lable = np.array(self.lable_list_all)\n",
    "        signal = np.array(self.signal_list_all)\n",
    "\n",
    "        unique_elements, counts_elements = np.unique(lable[:,0], return_counts=True)\n",
    "        \n",
    "        print(counts_elements)\n",
    "        print(unique_elements)\n",
    "        \n",
    "        for element in unique_elements:\n",
    "            \n",
    "            total_count = counts_elements[np.where(unique_elements == element)]\n",
    "            \n",
    "            count_80 = int(total_count * 0.8)\n",
    "            \n",
    "            arr_l_80, arr_l_20 = np.split(lable[(lable[:,0] == element)], [count_80])\n",
    "            arr_s_80, arr_s_20 = np.split(signal[(lable[:,0] == element)], [count_80])\n",
    "\n",
    "            for count in range(len(arr_l_80)):\n",
    "                self.lable_list_80.append(arr_l_80[count])\n",
    "                \n",
    "            for count in range(len(arr_l_20)):   \n",
    "                self.lable_list_20.append(arr_l_20[count])\n",
    "                \n",
    "            for count in range(len(arr_l_80)):\n",
    "                self.signal_list_80.append(arr_s_80[count])\n",
    "                \n",
    "            for count in range(len(arr_l_20)):   \n",
    "                self.signal_list_20.append(arr_s_20[count])     \n",
    "                \n",
    "    def get_dataset_80(self):   \n",
    "        return self.signal_list_80, self.lable_list_80\n",
    "        \n",
    "    def get_dataset_20(self):   \n",
    "        return self.signal_list_20, self.lable_list_20\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "347a6384-6aa6-4216-8bb9-6701497bf15f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, signal_list, lable_list):\n",
    "\n",
    "        self.lable_list_ = lable_list\n",
    "        self.signal_list_ = signal_list\n",
    "        self.lable_list = []\n",
    "        self.signal_list = []\n",
    "        \n",
    "        self.append_data()\n",
    "\n",
    "    def append_data(self):\n",
    "        \n",
    "        lable = np.array(self.lable_list_)\n",
    "        signal = np.array(self.signal_list_)\n",
    "\n",
    "        lable_ = torch.from_numpy(lable).float().to(device)\n",
    "        signal_ = torch.from_numpy(signal).float().to(device)\n",
    "        \n",
    "        for value in lable_:\n",
    "            self.lable_list.append(value)\n",
    "            \n",
    "        for value in signal_:\n",
    "            self.signal_list.append(value)\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.lable_list) \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.signal_list[idx], self.lable_list[idx]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7e1f8b4-93cc-4a57-839a-f70320badf09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  6  20  50  49  65 113 160 230 224 251 206 220 106 223 221 157 120 207\n",
      " 167 118 131  71  76 113  19  10   9   9   7   2]\n",
      "[63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86\n",
      " 87 88 89 90 91 92]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "root_folder_train = \"data\"\n",
    "dataset = Load_Data(root_folder_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eacf7210-7317-45f5-8fba-761d10185ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "signal_list_80, lable_list_80 = dataset.get_dataset_80()\n",
    "dataset_train = MyDataset(signal_list_80, lable_list_80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d3a87b8-b2bf-41df-8e18-20304e6607b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "signal_list_20, lable_list_20 = dataset.get_dataset_20()\n",
    "dataset_test = MyDataset(signal_list_20, lable_list_20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2c27089-8e16-4892-baab-a3b300b9c317",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f2f91c1-803b-43f0-8fa0-86552b9acc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([32, 1, 1500])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 1])\n",
      "next\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i_batch, (inputs, targets) in enumerate(dataloader_train):\n",
    "\n",
    "    print(i_batch)\n",
    "    print(inputs.shape)\n",
    "    print(targets.shape)\n",
    "    print(targets[:,0:1].shape)\n",
    "    print(\"next\")\n",
    "\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cea146b-d796-4378-a7c5-2f6783beb9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8d8db1f-ff06-4e62-a586-c8d0f7de18bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([32, 1, 1500])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 1])\n",
      "next\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i_batch, (inputs, targets) in enumerate(dataloader_test):\n",
    "\n",
    "    print(i_batch)\n",
    "    print(inputs.shape)\n",
    "    print(targets.shape)\n",
    "    print(targets[:,0:1].shape)\n",
    "    print(\"next\")\n",
    "\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e59323b-77c4-4e5c-803f-4395b53a0d0c",
   "metadata": {},
   "source": [
    "## Einlesen der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "758b3871-0eff-4d39-9127-1d5d81020161",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Pulse_Respiration_CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 input_width, \n",
    "                 conv_layer_, \n",
    "                 fc_layers_, \n",
    "                 conv_channel_, \n",
    "                 conv_kernel_, \n",
    "                 conv_stride_,\n",
    "                 pool_kernel_, \n",
    "                 pool_stride_,\n",
    "                 fully_connected_):\n",
    "        \n",
    "        super(Pulse_Respiration_CNN, self).__init__()\n",
    "\n",
    "        self.af_1 = nn.Tanh()\n",
    "        self.af_2 = nn.ReLU()\n",
    "\n",
    "        self.conv_1_layers = nn.ModuleList()\n",
    "        self.bn_1_layers = nn.ModuleList()\n",
    "        self.pool_1_layers = nn.ModuleList()\n",
    "        \n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        \n",
    "        conv_layer = conv_layer_\n",
    "        fc_layers = fc_layers_\n",
    "        \n",
    "        print(f\"conv_layer: {conv_layer}\")\n",
    "        print(f\"fc_layers: {fc_layers}\")\n",
    "\n",
    "        Width = input_width\n",
    "        in_channels = 1\n",
    "\n",
    "        for i in range(conv_layer):\n",
    "            \n",
    "            conv_channel = conv_channel_[i]\n",
    "            conv_kernel = conv_kernel_[i]\n",
    "            conv_stride = conv_stride_[i]\n",
    "            \n",
    "            pool_kernel = pool_kernel_[i]\n",
    "            pool_stride = pool_stride_[i]\n",
    "     \n",
    "            Kernel = conv_kernel # kernel_size\n",
    "            Padding = 0 # (Kernel - 1) // 2 # padding\n",
    "            Stride = conv_stride # stride\n",
    "            Width_a = ((Width - conv_kernel + 2 * Padding) // conv_stride) + 1\n",
    "            \n",
    "            Width_b = ((Width_a - pool_kernel) // pool_stride) + 1\n",
    "            \n",
    "            if (Width_b < 4):\n",
    "                Kernel = conv_kernel * conv_stride + pool_kernel * pool_stride\n",
    "            \n",
    "                Padding = (Kernel - 1) // 2 # padding\n",
    "                Width = ((Width - conv_kernel + 2 * Padding) // conv_stride) + 1\n",
    "                Width = ((Width - pool_kernel) // pool_stride) + 1\n",
    "            else:\n",
    "                Width = Width_b\n",
    "            \n",
    "            self.conv_1_layers.append(nn.Conv1d(in_channels=in_channels, out_channels=conv_channel, kernel_size=conv_kernel, stride=conv_stride, padding=Padding))\n",
    "            self.bn_1_layers.append(nn.BatchNorm1d(conv_channel))\n",
    "            self.pool_1_layers.append(nn.MaxPool1d(pool_kernel, pool_stride)) \n",
    "            in_channels = conv_channel\n",
    "            \n",
    "            print(f\"s{i}_conv_channel_1: {conv_channel} s{i}_conv_kernel_1: {conv_kernel} s{i}_conv_stride_1: {conv_stride} s{i}_padding_1: {Padding} s{i}_pool_kernel_1: {pool_kernel} s{i}_pool_stride_1: {pool_stride} s{i}_Width_1: {Width}\")\n",
    "        \n",
    "        out_features = Width * in_channels\n",
    "        \n",
    "        print(f\"s{i}_features: {out_features} = Width: {Width} * in_channels: {in_channels}\")\n",
    "\n",
    "        for i in range(fc_layers):\n",
    "            \n",
    "            out_features_next = fully_connected_[i]\n",
    "            self.fc_layers.append(nn.Linear(in_features=out_features, out_features=out_features_next))\n",
    "            out_features = out_features_next\n",
    "            \n",
    "            print(f\"s{i}_fully_connected_: {out_features_next}\")\n",
    "            \n",
    "        self.fc_out = nn.Linear(in_features=out_features, out_features=1)    \n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        for conv_1, bn_1, pool_1 in zip(self.conv_1_layers, self.bn_1_layers, self.pool_1_layers):\n",
    "            \n",
    "            x = conv_1(x)\n",
    "            x = bn_1(x)\n",
    "            x = self.af_1(x)\n",
    "            x = pool_1(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        for layer_1 in self.fc_layers:\n",
    "            \n",
    "            x = layer_1(x)\n",
    "            x = self.af_2(x)\n",
    "            \n",
    "        x = self.fc_out(x)\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d101f963-b750-4d50-b67b-c056b92f77d1",
   "metadata": {},
   "source": [
    "## Modellinstanz erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eac2c3be-02c3-4a1c-b609-5b82adbc0281",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "\n",
    "    for i_batch, (inputs, targets) in enumerate(dataloader_train):\n",
    "        input_width_ = inputs.shape[2]\n",
    "        break\n",
    "\n",
    "    conv_layer = trial.suggest_int(\"conv_layer_1\", 8, 13)\n",
    "    fc_layers = trial.suggest_int(\"fc_layers_1\", 1, 3)\n",
    "    \n",
    "    conv_channel = []\n",
    "    conv_kernel = []\n",
    "    conv_stride = []\n",
    "    pool_kernel = []\n",
    "    pool_stride = []\n",
    "    \n",
    "    fully_connected = []\n",
    "    \n",
    "    for i in range(conv_layer):\n",
    "\n",
    "        conv_channel.append(trial.suggest_categorical(\"conv_channel_1_{}\".format(i), [16, 32, 64])) # conv_channel\n",
    "        conv_kernel.append(trial.suggest_categorical(\"conv_kernel_1_{}\".format(i), [3, 5, 7, 17, 29])) # conv_kernel\n",
    "        conv_stride.append(1) # conv_stride.append(trial.suggest_categorical(\"conv_stride_1_{}\".format(i), [1, 2, 3]) # conv_stride)\n",
    "\n",
    "        pool_kernel.append(trial.suggest_categorical(\"pool_kernel_1_{}\".format(i), [3, 4, 5])) # pool_kernel\n",
    "        pool_stride.append(trial.suggest_categorical(\"pool_stride_1_{}\".format(i), [1, 2])) # pool_stride\n",
    "\n",
    "    for i in range(fc_layers):\n",
    "    \n",
    "        fully_connected.append(trial.suggest_categorical(\"fully_connected_{}\".format(i), [32, 64, 256, 1024]))\n",
    "\n",
    "    model = Pulse_Respiration_CNN(input_width = input_width_, \n",
    "                                  conv_layer_ = conv_layer, \n",
    "                                  fc_layers_ = fc_layers, \n",
    "                                  conv_channel_ = conv_channel, \n",
    "                                  conv_kernel_ = conv_kernel, \n",
    "                                  conv_stride_ = conv_stride,\n",
    "                                  pool_kernel_ = pool_kernel, \n",
    "                                  pool_stride_ = pool_stride,\n",
    "                                  fully_connected_ = fully_connected).to(device)\n",
    "                    \n",
    "    learning_rate = 0.001\n",
    "    L2_regularization = 1e-5 # recommended = 1e-5 # default = 0\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=L2_regularization) \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    epochs = 100\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for i_batch, (inputs, targets) in enumerate(dataloader_train):\n",
    "\n",
    "            optimizer.zero_grad() \n",
    "            outputs = model(inputs) \n",
    "            loss = criterion(outputs, targets[:,0:1]) # breathing and heart\n",
    "            loss.backward() \n",
    "            optimizer.step() \n",
    "\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        train_loss = train_loss /len(dataloader_train)\n",
    "\n",
    "        model.eval() \n",
    "        valid_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for i_batch, (inputs, targets) in enumerate(dataloader_test):\n",
    "\n",
    "                outputs = model(inputs) \n",
    "                loss = criterion(outputs, targets[:,0:1]) \n",
    "                valid_loss += loss.item()\n",
    "\n",
    "            valid_loss = valid_loss /len(dataloader_test)\n",
    "            \n",
    "        trial.report(valid_loss, epoch)\n",
    "\n",
    "        if trial.should_prune():\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "            \n",
    "        if (epoch + 3) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch + 3}/{epochs}], Run Loss: {train_loss:.4f}, Val Loss: {valid_loss:.4f}\")\n",
    "        \n",
    "        if (epoch + 3) % 500 == 0:\n",
    "            print(f\"predictions [{outputs.cpu().numpy()[0]}], targets: {targets[:,0:1].cpu().numpy()[0]}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    execution_time = end_time - start_time\n",
    "    \n",
    "    print(\"Die Ausführungszeit beträgt: \", execution_time, \" Sekunden\")\n",
    "    \n",
    "    return valid_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d614b8f-a57b-4bd5-8b82-2bb533abad5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-08-07 12:05:01,483] A new study created in RDB with name: 1d_cnn_model_try\n"
     ]
    }
   ],
   "source": [
    "\n",
    "pruner = optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=20, interval_steps=1) # n_startup_trials = 5, n_warmup_steps = 0\n",
    "study = optuna.create_study(study_name=model_name, storage=\"sqlite:///\"+model_name+\".db\", load_if_exists=True, pruner=pruner)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51828aed-547a-47e5-96ca-c711b56b9d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_layer: 8\n",
      "fc_layers: 3\n",
      "s0_conv_channel_1: 16 s0_conv_kernel_1: 5 s0_conv_stride_1: 1 s0_padding_1: 0 s0_pool_kernel_1: 4 s0_pool_stride_1: 2 s0_Width_1: 747\n",
      "s1_conv_channel_1: 64 s1_conv_kernel_1: 5 s1_conv_stride_1: 1 s1_padding_1: 0 s1_pool_kernel_1: 5 s1_pool_stride_1: 2 s1_Width_1: 370\n",
      "s2_conv_channel_1: 64 s2_conv_kernel_1: 7 s2_conv_stride_1: 1 s2_padding_1: 0 s2_pool_kernel_1: 5 s2_pool_stride_1: 1 s2_Width_1: 360\n",
      "s3_conv_channel_1: 16 s3_conv_kernel_1: 29 s3_conv_stride_1: 1 s3_padding_1: 0 s3_pool_kernel_1: 5 s3_pool_stride_1: 1 s3_Width_1: 328\n",
      "s4_conv_channel_1: 64 s4_conv_kernel_1: 17 s4_conv_stride_1: 1 s4_padding_1: 0 s4_pool_kernel_1: 5 s4_pool_stride_1: 2 s4_Width_1: 154\n",
      "s5_conv_channel_1: 16 s5_conv_kernel_1: 3 s5_conv_stride_1: 1 s5_padding_1: 0 s5_pool_kernel_1: 3 s5_pool_stride_1: 2 s5_Width_1: 75\n",
      "s6_conv_channel_1: 32 s6_conv_kernel_1: 3 s6_conv_stride_1: 1 s6_padding_1: 0 s6_pool_kernel_1: 4 s6_pool_stride_1: 2 s6_Width_1: 35\n",
      "s7_conv_channel_1: 16 s7_conv_kernel_1: 5 s7_conv_stride_1: 1 s7_padding_1: 0 s7_pool_kernel_1: 3 s7_pool_stride_1: 1 s7_Width_1: 29\n",
      "s7_features: 464 = Width: 29 * in_channels: 16\n",
      "s0_fully_connected_: 64\n",
      "s1_fully_connected_: 256\n",
      "s2_fully_connected_: 32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "study.optimize(objective, n_trials=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463260a2-5b23-4470-9c30-0fa43c7bc1c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Best trial:\", study.best_trial.value)\n",
    "print(\"Best parameters:\", study.best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d0aa4f-6d02-41ee-b2a9-4f8b75dcc154",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "fig = optuna.visualization.plot_intermediate_values(study)\n",
    "fig.update_layout(yaxis_type=\"log\")\n",
    "plotly.offline.plot(fig, filename=\"1d_cnn_model_try_Intermediate_Values.html\", image=\"svg\", image_filename=\"1d_cnn_model_try_Intermediate_Values\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e07c257-4587-4473-b58f-ef9457e64421",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "fig = optuna.visualization.plot_optimization_history(study)\n",
    "plotly.offline.plot(fig, filename=\"1d_cnn_model_try_Hyperparameter_History.html\", image=\"svg\", image_filename=\"1d_cnn_model_try_Hyperparameter_History\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd1fea30-5b77-4c53-afc7-f23426a9ce25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
