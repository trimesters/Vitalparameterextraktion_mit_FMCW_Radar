{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08c2fcff-1d84-46b1-abba-387216d87457",
   "metadata": {},
   "source": [
    "# Algorithmus für künstliche Intelligenz mit CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60dc300-a77c-4fbc-9740-a019f619e386",
   "metadata": {},
   "source": [
    "## Libaray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45d3a56a-304d-4e05-a93e-862cd647b9c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import optuna\n",
    "\n",
    "import plotly\n",
    "import plotly.io as pio\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a91d2136-47c8-4570-82ae-6281da1afbae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "torch.set_float32_matmul_precision(\"high\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a6e9b2-9d7d-4bdc-a7fb-17f7e73ba928",
   "metadata": {},
   "source": [
    "## Modell auf der GPU ausführen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ad082b2-e2d6-4794-99ca-754588e54b1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4a867a-9cb3-468e-8368-82281bcb3b64",
   "metadata": {},
   "source": [
    "## Modell Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0cd0c4b-47a0-4b99-8b5f-e423919bdcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_name = \"1d_cnn_model_test\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae871c7-68b7-43ba-aa9a-44dc5c92d040",
   "metadata": {},
   "source": [
    "## Einlesen der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c974fe44-ee94-49ec-a91b-56e57ca1a7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Load_Data:\n",
    "    \n",
    "    def __init__(self, root_dir = \"\"):\n",
    "        \n",
    "        self.root_dir = root_dir\n",
    "        \n",
    "        self.signal_list_all = []\n",
    "        self.lable_list_all = []\n",
    "        \n",
    "        self.signal_list_80 = []\n",
    "        self.signal_list_20 = []\n",
    "        self.lable_list_80 = []\n",
    "        self.lable_list_20 = []\n",
    "        \n",
    "        self.load_data()\n",
    "        self.sort_data()\n",
    "        \n",
    "    def load_data(self):\n",
    "        \n",
    "        for dirpath, dirnames, filenames in os.walk(self.root_dir):\n",
    "            for filename in filenames:\n",
    "                if filename.endswith(\".npz\"): \n",
    "                    \n",
    "                    file_path = os.path.join(dirpath, filename)\n",
    "                    data = np.load(file_path)\n",
    "\n",
    "                    signal_ = data[\"signal\"]\n",
    "                    signal_full = signal_[np.newaxis] # An additional dimension is needed for later processing\n",
    "                    signal = signal_full[:,::]  # Halving of the input data and thus of the features\n",
    "                    \n",
    "                    heart_rate = data[\"heart_uart\"]\n",
    "                    respiration_rate = data[\"respiration_radar\"]\n",
    "                    \n",
    "                    heart_rate_t = heart_rate\n",
    "                    respiration_rate_t = respiration_rate\n",
    "                    signal_t = signal\n",
    "   \n",
    "                    self.lable_list_all.append([heart_rate_t, respiration_rate_t])\n",
    "                    self.signal_list_all.append(signal_t)\n",
    "    \n",
    "    def sort_data(self):   \n",
    "        \n",
    "        lable = np.array(self.lable_list_all)\n",
    "        signal = np.array(self.signal_list_all)\n",
    "\n",
    "        unique_elements, counts_elements = np.unique(lable[:,0], return_counts=True)\n",
    "        \n",
    "        print(counts_elements)\n",
    "        print(unique_elements)\n",
    "        \n",
    "        for element in unique_elements:\n",
    "            \n",
    "            total_count = counts_elements[np.where(unique_elements == element)]\n",
    "            \n",
    "            count_80 = int(total_count * 0.8)\n",
    "            \n",
    "            arr_l_80, arr_l_20 = np.split(lable[(lable[:,0] == element)], [count_80])\n",
    "            arr_s_80, arr_s_20 = np.split(signal[(lable[:,0] == element)], [count_80])\n",
    "\n",
    "            for count in range(len(arr_l_80)):\n",
    "                self.lable_list_80.append(arr_l_80[count])\n",
    "                \n",
    "            for count in range(len(arr_l_20)):   \n",
    "                self.lable_list_20.append(arr_l_20[count])\n",
    "                \n",
    "            for count in range(len(arr_l_80)):\n",
    "                self.signal_list_80.append(arr_s_80[count])\n",
    "                \n",
    "            for count in range(len(arr_l_20)):   \n",
    "                self.signal_list_20.append(arr_s_20[count])     \n",
    "                \n",
    "    def get_dataset_80(self):   \n",
    "        return self.signal_list_80, self.lable_list_80\n",
    "        \n",
    "    def get_dataset_20(self):   \n",
    "        return self.signal_list_20, self.lable_list_20\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "347a6384-6aa6-4216-8bb9-6701497bf15f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class MyDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, signal_list, lable_list):\n",
    "\n",
    "        self.lable_list_ = lable_list\n",
    "        self.signal_list_ = signal_list\n",
    "        self.lable_list = []\n",
    "        self.signal_list = []\n",
    "        \n",
    "        self.append_data()\n",
    "\n",
    "    def append_data(self):\n",
    "        \n",
    "        lable = np.array(self.lable_list_)\n",
    "        signal = np.array(self.signal_list_)\n",
    "\n",
    "        lable_ = torch.from_numpy(lable).float().to(device)\n",
    "        signal_ = torch.from_numpy(signal).float().to(device)\n",
    "        \n",
    "        for value in lable_:\n",
    "            self.lable_list.append(value)\n",
    "            \n",
    "        for value in signal_:\n",
    "            self.signal_list.append(value)\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "\n",
    "        return len(self.lable_list) \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        return self.signal_list[idx], self.lable_list[idx]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7e1f8b4-93cc-4a57-839a-f70320badf09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  4  49  25  27  53  48  34  66  86 144  96  90 122 199 231 234 257 209\n",
      " 220 106 223 221 157 120 207 167 118 131  71  76 113  19  10   9   9   7\n",
      "   2]\n",
      "[56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79\n",
      " 80 81 82 83 84 85 86 87 88 89 90 91 92]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "root_folder_train = \"data\"\n",
    "dataset = Load_Data(root_folder_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eacf7210-7317-45f5-8fba-761d10185ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "signal_list_80, lable_list_80 = dataset.get_dataset_80()\n",
    "dataset_train = MyDataset(signal_list_80, lable_list_80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d3a87b8-b2bf-41df-8e18-20304e6607b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "signal_list_20, lable_list_20 = dataset.get_dataset_20()\n",
    "dataset_test = MyDataset(signal_list_20, lable_list_20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2c27089-8e16-4892-baab-a3b300b9c317",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f2f91c1-803b-43f0-8fa0-86552b9acc95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([32, 1, 1500])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 1])\n",
      "next\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i_batch, (inputs, targets) in enumerate(dataloader_train):\n",
    "\n",
    "    print(i_batch)\n",
    "    print(inputs.shape)\n",
    "    print(targets.shape)\n",
    "    print(targets[:,0:1].shape)\n",
    "    print(\"next\")\n",
    "\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cea146b-d796-4378-a7c5-2f6783beb9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=32, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f8d8db1f-ff06-4e62-a586-c8d0f7de18bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "torch.Size([32, 1, 1500])\n",
      "torch.Size([32, 2])\n",
      "torch.Size([32, 1])\n",
      "next\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i_batch, (inputs, targets) in enumerate(dataloader_test):\n",
    "\n",
    "    print(i_batch)\n",
    "    print(inputs.shape)\n",
    "    print(targets.shape)\n",
    "    print(targets[:,0:1].shape)\n",
    "    print(\"next\")\n",
    "\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e59323b-77c4-4e5c-803f-4395b53a0d0c",
   "metadata": {},
   "source": [
    "## Einlesen der Daten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "758b3871-0eff-4d39-9127-1d5d81020161",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Pulse_Respiration_CNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 input_width, \n",
    "                 conv_layer_, \n",
    "                 fc_layers_, \n",
    "                 conv_channel_, \n",
    "                 conv_kernel_, \n",
    "                 pool_kernel_, \n",
    "                 pool_stride_,\n",
    "                 fully_connected_):\n",
    "        \n",
    "        super(Pulse_Respiration_CNN, self).__init__()\n",
    "\n",
    "        self.af_1 = nn.Tanh()\n",
    "        self.af_2 = nn.ReLU()\n",
    "\n",
    "        self.conv_1_layers = nn.ModuleList()\n",
    "        self.bn_1_layers = nn.ModuleList()\n",
    "        self.pool_1_layers = nn.ModuleList()\n",
    "        \n",
    "        self.fc_layers = nn.ModuleList()\n",
    "        \n",
    "        conv_layer = conv_layer_\n",
    "        fc_layers = fc_layers_\n",
    "        \n",
    "        print(f\"conv_layer: {conv_layer}\")\n",
    "        print(f\"fc_layers: {fc_layers}\")\n",
    "\n",
    "        Width = input_width\n",
    "        in_channels = 1\n",
    "\n",
    "        for i in range(conv_layer):\n",
    "            \n",
    "            conv_channel = conv_channel_[i]\n",
    "            conv_kernel = conv_kernel_[i]\n",
    "            conv_stride = 1 \n",
    "            \n",
    "            pool_kernel = pool_kernel_[i]\n",
    "            pool_stride = pool_stride_[i]\n",
    "     \n",
    "            Kernel = conv_kernel # kernel_size\n",
    "            Padding = 0 # (Kernel - 1) // 2 # padding\n",
    "            Stride = conv_stride # stride\n",
    "            Width_a = ((Width - conv_kernel + 2 * Padding) // conv_stride) + 1\n",
    "            \n",
    "            Width_b = ((Width_a - pool_kernel) // pool_stride) + 1\n",
    "            \n",
    "            if (Width_b < 8):\n",
    "                Kernel = conv_kernel + pool_kernel\n",
    "            \n",
    "                Padding = (Kernel - 1) // 2 # padding\n",
    "                Width = ((Width - conv_kernel + 2 * Padding) // conv_stride) + 1\n",
    "                Width = ((Width - pool_kernel) // pool_stride) + 1\n",
    "            else:\n",
    "                Width = Width_b\n",
    "            \n",
    "            self.conv_1_layers.append(nn.Conv1d(in_channels=in_channels, out_channels=conv_channel, kernel_size=conv_kernel, stride=conv_stride, padding=Padding))\n",
    "            in_channels = conv_channel\n",
    "            \n",
    "            self.bn_1_layers.append(nn.BatchNorm1d(conv_channel))\n",
    "\n",
    "            self.pool_1_layers.append(nn.MaxPool1d(pool_kernel, pool_stride)) \n",
    "            \n",
    "            \n",
    "            print(f\"s{i}_conv_channel_1: {conv_channel} s{i}_conv_kernel_1: {conv_kernel} s{i}_conv_stride_1: {conv_stride} s{i}_padding_1: {Padding} s{i}_pool_kernel_1: {pool_kernel} s{i}_pool_stride_1: {pool_stride} s{i}_Width_1: {Width}\")\n",
    "        \n",
    "        out_features = Width * in_channels\n",
    "        \n",
    "        print(f\"s{i}_features: {out_features} = Width: {Width} * in_channels: {in_channels}\")\n",
    "\n",
    "        for i in range(fc_layers):\n",
    "            \n",
    "            out_features_next = fully_connected_[i]\n",
    "            \n",
    "            print(f\"s{i}_fully_connected_: {out_features_next}\")\n",
    "            \n",
    "            self.fc_layers.append(nn.Linear(in_features=out_features, out_features=out_features_next))\n",
    "            out_features = out_features_next\n",
    "            \n",
    "        self.fc_out = nn.Linear(in_features=out_features, out_features=1)    \n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        for conv_1, bn_1, pool_1 in zip(self.conv_1_layers, self.bn_1_layers, self.pool_1_layers):\n",
    "            \n",
    "            x = conv_1(x)\n",
    "            x = bn_1(x)\n",
    "            x = self.af_1(x)\n",
    "            x = pool_1(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        for layer_1 in self.fc_layers:\n",
    "            \n",
    "            x = layer_1(x)\n",
    "            x = self.af_2(x)\n",
    "            \n",
    "        x = self.fc_out(x)\n",
    "        \n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d101f963-b750-4d50-b67b-c056b92f77d1",
   "metadata": {},
   "source": [
    "## Modellinstanz erstellen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5657fd8-bc74-4319-95ea-e3173bb1deae",
   "metadata": {},
   "source": [
    "### Parameter \n",
    "\n",
    "##### conv_layer: 8\n",
    "##### fc_layers: 2\n",
    "##### s0_conv_channel_1: 16 s0_conv_kernel_1: 7 s0_conv_stride_1: 1 s0_padding_1: 0 s0_pool_kernel_1: 5 s0_pool_stride_1: 2 s0_Width_1: 745\n",
    "##### s1_conv_channel_1: 16 s1_conv_kernel_1: 29 s1_conv_stride_1: 1 s1_padding_1: 0 s1_pool_kernel_1: 5 s1_pool_stride_1: 1 s1_Width_1: 713\n",
    "##### s2_conv_channel_1: 16 s2_conv_kernel_1: 3 s2_conv_stride_1: 1 s2_padding_1: 0 s2_pool_kernel_1: 3 s2_pool_stride_1: 1 s2_Width_1: 709\n",
    "##### s3_conv_channel_1: 32 s3_conv_kernel_1: 29 s3_conv_stride_1: 1 s3_padding_1: 0 s3_pool_kernel_1: 4 s3_pool_stride_1: 1 s3_Width_1: 678\n",
    "##### s4_conv_channel_1: 16 s4_conv_kernel_1: 3 s4_conv_stride_1: 1 s4_padding_1: 0 s4_pool_kernel_1: 4 s4_pool_stride_1: 1 s4_Width_1: 673\n",
    "##### s5_conv_channel_1: 64 s5_conv_kernel_1: 29 s5_conv_stride_1: 1 s5_padding_1: 0 s5_pool_kernel_1: 3 s5_pool_stride_1: 1 s5_Width_1: 643\n",
    "##### s6_conv_channel_1: 32 s6_conv_kernel_1: 7 s6_conv_stride_1: 1 s6_padding_1: 0 s6_pool_kernel_1: 4 s6_pool_stride_1: 1 s6_Width_1: 634\n",
    "##### s7_conv_channel_1: 16 s7_conv_kernel_1: 17 s7_conv_stride_1: 1 s7_padding_1: 0 s7_pool_kernel_1: 5 s7_pool_stride_1: 1 s7_Width_1: 614\n",
    "##### s7_features: 9824 = Width: 614 * in_channels: 16\n",
    "##### s0_fully_connected_: 32\n",
    "##### s1_fully_connected_: 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eac2c3be-02c3-4a1c-b609-5b82adbc0281",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_model():\n",
    "    \n",
    "    start_time = time.time()\n",
    "\n",
    "    for i_batch, (inputs, targets) in enumerate(dataloader_train):\n",
    "        input_s_ = inputs.shape[2]\n",
    "        \n",
    "    conv_layer = 8\n",
    "    fc_layers = 2\n",
    "    \n",
    "    conv_channel = [16,16,16,32,16,64,32,16]\n",
    "    conv_kernel = [5,29,3,29,3,29,7,17]\n",
    "    pool_kernel = [5,5,3,4,4,3,4,5]\n",
    "    pool_stride = [2,1,1,1,1,1,1,1]\n",
    "    \n",
    "    fully_connected = [32,32]\n",
    "\n",
    "    model = Pulse_Respiration_CNN(input_width = input_s_, \n",
    "                                  conv_layer_ = conv_layer, \n",
    "                                  fc_layers_ = fc_layers, \n",
    "                                  conv_channel_ = conv_channel, \n",
    "                                  conv_kernel_ = conv_kernel, \n",
    "                                  pool_kernel_ = pool_kernel, \n",
    "                                  pool_stride_ = pool_stride,\n",
    "                                  fully_connected_ = fully_connected).to(device)\n",
    "\n",
    "    dateipfad = model_name\n",
    "    \n",
    "    if os.path.isfile(dateipfad):\n",
    "        print(\"Die Datei existiert.\")\n",
    "        model.load_state_dict(torch.load(model_name))\n",
    "        \n",
    "    \n",
    "    learning_rate = 0.001\n",
    "    L2_regularization = 1e-5 # recommended = 1e-5 # default = 0\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=L2_regularization) \n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    epochs = 1000\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "\n",
    "        for i_batch, (inputs, targets) in enumerate(dataloader_train):\n",
    "\n",
    "            optimizer.zero_grad() \n",
    "            outputs = model(inputs) \n",
    "            loss = criterion(outputs, targets[:,0:1]) # breathing and heart\n",
    "            loss.backward() \n",
    "            optimizer.step() \n",
    "\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "        train_loss = train_loss /len(dataloader_train)\n",
    "            \n",
    "        model.eval() \n",
    "        valid_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for i_batch, (inputs, targets) in enumerate(dataloader_test):\n",
    "\n",
    "                outputs = model(inputs) \n",
    "                loss = criterion(outputs, targets[:,0:1]) \n",
    "                valid_loss += loss.item()\n",
    "\n",
    "            valid_loss = valid_loss /len(dataloader_test)\n",
    "\n",
    "        if (epoch + 3) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch + 3}/{epochs}], Run Loss: {train_loss:.4f}, Val Loss: {valid_loss:.4f}\")\n",
    "        \n",
    "        if (epoch + 2) % 1000 == 0:\n",
    "            print(f\"predictions [{outputs.cpu().numpy()[0]}], targets: {targets[:,0:1].cpu().numpy()[0]}\")\n",
    "\n",
    "        if (epoch + 1) % 1000 == 0:\n",
    "            torch.save(model.state_dict(), model_name + \".pt\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    execution_time = end_time - start_time\n",
    "    \n",
    "    print(\"Die Ausführungszeit beträgt: \", execution_time, \" Sekunden\")\n",
    "    \n",
    "    return valid_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3d614b8f-a57b-4bd5-8b82-2bb533abad5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_layer: 8\n",
      "fc_layers: 2\n",
      "s0_conv_channel_1: 32 s0_conv_kernel_1: 3 s0_conv_stride_1: 1 s0_padding_1: 0 s0_pool_kernel_1: 3 s0_pool_stride_1: 1 s0_Width_1: 1496\n",
      "s1_conv_channel_1: 16 s1_conv_kernel_1: 5 s1_conv_stride_1: 1 s1_padding_1: 0 s1_pool_kernel_1: 3 s1_pool_stride_1: 2 s1_Width_1: 745\n",
      "s2_conv_channel_1: 32 s2_conv_kernel_1: 17 s2_conv_stride_1: 1 s2_padding_1: 0 s2_pool_kernel_1: 3 s2_pool_stride_1: 3 s2_Width_1: 243\n",
      "s3_conv_channel_1: 16 s3_conv_kernel_1: 29 s3_conv_stride_1: 1 s3_padding_1: 0 s3_pool_kernel_1: 4 s3_pool_stride_1: 1 s3_Width_1: 212\n",
      "s4_conv_channel_1: 32 s4_conv_kernel_1: 3 s4_conv_stride_1: 1 s4_padding_1: 0 s4_pool_kernel_1: 4 s4_pool_stride_1: 2 s4_Width_1: 104\n",
      "s5_conv_channel_1: 16 s5_conv_kernel_1: 5 s5_conv_stride_1: 1 s5_padding_1: 0 s5_pool_kernel_1: 4 s5_pool_stride_1: 3 s5_Width_1: 33\n",
      "s6_conv_channel_1: 32 s6_conv_kernel_1: 17 s6_conv_stride_1: 1 s6_padding_1: 0 s6_pool_kernel_1: 5 s6_pool_stride_1: 1 s6_Width_1: 13\n",
      "s7_conv_channel_1: 16 s7_conv_kernel_1: 29 s7_conv_stride_1: 1 s7_padding_1: 16 s7_pool_kernel_1: 5 s7_pool_stride_1: 2 s7_Width_1: 7\n",
      "s7_features: 112 = Width: 7 * in_channels: 16\n",
      "s0_fully_connected_: 128\n",
      "s1_fully_connected_: 32\n",
      "Epoch [100/4000], Run Loss: 0.8360, Val Loss: 9.8270\n",
      "Epoch [200/4000], Run Loss: 0.5154, Val Loss: 11.5234\n",
      "Epoch [300/4000], Run Loss: 0.2254, Val Loss: 10.3744\n",
      "Epoch [400/4000], Run Loss: 0.4265, Val Loss: 8.8393\n",
      "Epoch [500/4000], Run Loss: 0.1759, Val Loss: 10.0812\n",
      "Epoch [600/4000], Run Loss: 0.1531, Val Loss: 8.6419\n",
      "Epoch [700/4000], Run Loss: 0.1494, Val Loss: 9.1139\n",
      "Epoch [800/4000], Run Loss: 0.1785, Val Loss: 9.4671\n",
      "Epoch [900/4000], Run Loss: 0.1098, Val Loss: 8.4706\n",
      "Epoch [1000/4000], Run Loss: 0.1128, Val Loss: 8.9084\n",
      "predictions [[86.93069]], targets: [85.]\n",
      "Epoch [1100/4000], Run Loss: 0.0886, Val Loss: 8.1708\n",
      "Epoch [1200/4000], Run Loss: 0.0787, Val Loss: 9.7780\n",
      "Epoch [1300/4000], Run Loss: 0.1231, Val Loss: 10.0570\n",
      "Epoch [1400/4000], Run Loss: 0.0704, Val Loss: 10.2860\n",
      "Epoch [1500/4000], Run Loss: 0.0867, Val Loss: 9.1713\n",
      "Epoch [1600/4000], Run Loss: 0.0601, Val Loss: 8.7766\n",
      "Epoch [1700/4000], Run Loss: 0.0568, Val Loss: 8.9352\n",
      "Epoch [1800/4000], Run Loss: 0.0548, Val Loss: 9.2450\n",
      "Epoch [1900/4000], Run Loss: 0.0835, Val Loss: 9.3332\n",
      "Epoch [2000/4000], Run Loss: 0.0392, Val Loss: 8.7727\n",
      "predictions [[76.60614]], targets: [64.]\n",
      "Epoch [2100/4000], Run Loss: 0.0629, Val Loss: 10.1100\n",
      "Epoch [2200/4000], Run Loss: 0.0551, Val Loss: 10.2626\n",
      "Epoch [2300/4000], Run Loss: 0.0663, Val Loss: 10.1782\n",
      "Epoch [2400/4000], Run Loss: 0.0692, Val Loss: 8.2590\n",
      "Epoch [2500/4000], Run Loss: 0.0535, Val Loss: 8.6039\n",
      "Epoch [2600/4000], Run Loss: 0.0601, Val Loss: 8.7697\n",
      "Epoch [2700/4000], Run Loss: 0.0490, Val Loss: 9.1525\n",
      "Epoch [2800/4000], Run Loss: 0.0598, Val Loss: 9.0970\n",
      "Epoch [2900/4000], Run Loss: 0.0351, Val Loss: 8.8330\n",
      "Epoch [3000/4000], Run Loss: 0.0451, Val Loss: 7.8499\n",
      "predictions [[87.77541]], targets: [91.]\n",
      "Epoch [3100/4000], Run Loss: 0.0464, Val Loss: 8.8895\n",
      "Epoch [3200/4000], Run Loss: 0.0350, Val Loss: 8.5917\n",
      "Epoch [3300/4000], Run Loss: 0.0532, Val Loss: 10.6881\n",
      "Epoch [3400/4000], Run Loss: 0.0469, Val Loss: 8.4850\n",
      "Epoch [3500/4000], Run Loss: 0.0464, Val Loss: 8.0761\n",
      "Epoch [3600/4000], Run Loss: 0.0332, Val Loss: 9.9075\n",
      "Epoch [3700/4000], Run Loss: 0.0480, Val Loss: 8.9480\n",
      "Epoch [3800/4000], Run Loss: 0.0378, Val Loss: 8.7013\n",
      "Epoch [3900/4000], Run Loss: 0.0328, Val Loss: 9.9839\n",
      "Epoch [4000/4000], Run Loss: 0.0662, Val Loss: 8.5850\n",
      "predictions [[76.41109]], targets: [69.]\n",
      "Die Ausführungszeit beträgt:  3215.0206451416016  Sekunden\n"
     ]
    }
   ],
   "source": [
    "\n",
    "valid_loss = train_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51828aed-547a-47e5-96ca-c711b56b9d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(valid_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463260a2-5b23-4470-9c30-0fa43c7bc1c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a5502b-e97e-4091-91e6-c5aa363f61ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
